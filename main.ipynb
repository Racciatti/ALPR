{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a4GHO8rkmS-G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 03:21:04.072667: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-31 03:21:04.279747: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-31 03:21:04.371084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753942864.524829    2090 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753942864.560867    2090 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753942864.809202    2090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753942864.809241    2090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753942864.809245    2090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753942864.809249    2090 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-31 03:21:04.827776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, datasets, Model\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1434,
     "status": "ok",
     "timestamp": 1710973724243,
     "user": {
      "displayName": "Enzo Nicolás Stromberg Racciatti",
      "userId": "12238508451821531070"
     },
     "user_tz": 180
    },
    "id": "tAXTbXe8T6yW",
    "outputId": "beb8a478-6000-4709-f10e-4747c814bb10"
   },
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m 3252224/11490434\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1us/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# EXEMPLO AQUI: É NESSE FORMATO QUE PRECISAMOS\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m (train_images, train_labels), (test_images, test_labels) = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmnist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Setting input shape, normalizing color channel, setting datatype to float32 for numerical stability\u001b[39;00m\n\u001b[32m      7\u001b[39m train_images = train_images.reshape((\u001b[32m60000\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m1\u001b[39m)).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m) / \u001b[32m255\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/datasets/mnist.py:60\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the MNIST dataset.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03mThis is a dataset of 60,000 28x28 grayscale images of the 10 digits,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \u001b[33;03m    https://creativecommons.org/licenses/by-sa/3.0/)\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m origin_folder = (\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://storage.googleapis.com/tensorflow/tf-keras-datasets/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m path = \u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin_folder\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmnist.npz\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.load(path, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     68\u001b[39m     x_train, y_train = f[\u001b[33m\"\u001b[39m\u001b[33mx_train\u001b[39m\u001b[33m\"\u001b[39m], f[\u001b[33m\"\u001b[39m\u001b[33my_train\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/utils/file_utils.py:311\u001b[39m, in \u001b[36mget_file\u001b[39m\u001b[34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    313\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg.format(origin, e.code, e.msg))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:268\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[32m    266\u001b[39m     reporthook(blocknum, bs, size)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m block := \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    269\u001b[39m     read += \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[32m    270\u001b[39m     tfp.write(block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# EXEMPLO AQUI: É NESSE FORMATO QUE PRECISAMOS\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Setting input shape, normalizing color channel, setting datatype to float32 for numerical stability\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "classes = [str(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image size reduction\n",
    "2.5 x reduction --> 6.25 pixel number reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24850 images to resize. Starting process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24850/24850 [00:10<00:00, 2463.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Successfully resized and saved 24850 images to ./data/resized/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_DIR = './data/train/'      # Folder with class subfolders (e.g., A, B, C...)\n",
    "OUTPUT_DIR = './data/resized/'    # Where to save the resized images\n",
    "NEW_SIZE = (40, 30)               # New size for the images (width, height)\n",
    "\n",
    "# --- 1. Function to resize and save a single image ---\n",
    "def resize_and_save(relative_path):\n",
    "    \"\"\"\n",
    "    Reads an image from its relative path, resizes it, and saves it to the\n",
    "    output directory while maintaining the subfolder structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the full input and output paths\n",
    "        full_input_path = os.path.join(SOURCE_DIR, relative_path)\n",
    "        full_output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "\n",
    "        # Create the destination subdirectory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(full_input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            # print(f\"Warning: Could not read {full_input_path}, skipping.\")\n",
    "            return 0 # Return 0 for failure\n",
    "\n",
    "        # Resize the image using INTER_AREA for best downscaling results\n",
    "        resized_image = cv2.resize(image, NEW_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Save the resized image\n",
    "        cv2.imwrite(full_output_path, resized_image)\n",
    "        return 1 # Return 1 for success\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {relative_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 2. Main execution block to run the process in parallel ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Find all image files in the source directory and its subdirectories\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(SOURCE_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                # Get the path reomlative to the source directory (e.g., \"A/image1.png\")\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), SOURCE_DIR)\n",
    "                all_image_paths.append(relative_path)\n",
    "    \n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {SOURCE_DIR}. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_image_paths)} images to resize. Starting process...\")\n",
    "\n",
    "        # Use ProcessPoolExecutor to run resizing on all available CPU cores\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Use tqdm to create a progress bar\n",
    "            results = list(tqdm(executor.map(resize_and_save, all_image_paths), total=len(all_image_paths)))\n",
    "\n",
    "        total_processed = sum(results)\n",
    "        print(f\"\\n✅ Done! Successfully resized and saved {total_processed} images to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OTSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24850 images to threshold. Starting process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24850/24850 [00:09<00:00, 2718.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Successfully processed and saved 24850 images to ./data/thresholded/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use the resized images as the source for maximum efficiency\n",
    "SOURCE_DIR = './data/resized/'\n",
    "OUTPUT_DIR = './data/thresholded/'\n",
    "\n",
    "# --- 1. Function to threshold and save a single image ---\n",
    "def threshold_and_save(relative_path):\n",
    "    \"\"\"\n",
    "    Reads an image, applies Otsu's thresholding, and saves the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct full input and output paths\n",
    "        full_input_path = os.path.join(SOURCE_DIR, relative_path)\n",
    "        full_output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "\n",
    "        # Create the destination subdirectory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "        # Read the image in grayscale\n",
    "        image = cv2.imread(full_input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return 0  # Return 0 for failure\n",
    "\n",
    "        # Apply Otsu's thresholding\n",
    "        # The [1] selects the image array from the tuple returned by cv2.threshold\n",
    "        thresholded_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # Save the thresholded image\n",
    "        cv2.imwrite(full_output_path, thresholded_image)\n",
    "        return 1  # Return 1 for success\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {relative_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 2. Main execution block to run the process in parallel ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Find all image files in the source directory\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(SOURCE_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), SOURCE_DIR)\n",
    "                all_image_paths.append(relative_path)\n",
    "\n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {SOURCE_DIR}. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_image_paths)} images to threshold. Starting process...\")\n",
    "\n",
    "        # Use ProcessPoolExecutor to run on all available CPU cores\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            # Use tqdm to create a progress bar\n",
    "            results = list(tqdm(executor.map(threshold_and_save, all_image_paths), total=len(all_image_paths)))\n",
    "\n",
    "        total_processed = sum(results)\n",
    "        print(f\"\\n✅ Done! Successfully processed and saved {total_processed} images to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24850 images to process. Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24850/24850 [00:08<00:00, 2840.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Processed 24850 images saved to ./data/adaptive_thresholded/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_DIR = './data/resized/'\n",
    "OUTPUT_DIR = './data/adaptive_thresholded/'\n",
    "BLOCK_SIZE = 15  # Must be an odd number (e.g., 11, 15, 21)\n",
    "C_CONSTANT = 4   # Constant to subtract from the mean (fine-tune this)\n",
    "\n",
    "# --- 1. Function to apply adaptive thresholding and save ---\n",
    "def adaptive_threshold_and_save(relative_path):\n",
    "    \"\"\"\n",
    "    Reads an image, applies adaptive thresholding, and saves the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_input_path = os.path.join(SOURCE_DIR, relative_path)\n",
    "        full_output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "\n",
    "        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "        image = cv2.imread(full_input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return 0\n",
    "\n",
    "        # Apply Adaptive Thresholding\n",
    "        thresholded_image = cv2.adaptiveThreshold(\n",
    "            image,\n",
    "            255,                                  # Max value to assign\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,       # Method to calculate threshold\n",
    "            cv2.THRESH_BINARY_INV,                # Invert the image (character is white)\n",
    "            BLOCK_SIZE,                           # Neighborhood size\n",
    "            C_CONSTANT                            # Constant to subtract\n",
    "        )\n",
    "\n",
    "        cv2.imwrite(full_output_path, thresholded_image)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {relative_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 2. Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(SOURCE_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), SOURCE_DIR)\n",
    "                all_image_paths.append(relative_path)\n",
    "\n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {SOURCE_DIR}. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_image_paths)} images to process. Starting...\")\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(adaptive_threshold_and_save, all_image_paths), total=len(all_image_paths)))\n",
    "\n",
    "        total_processed = sum(results)\n",
    "        print(f\"\\n✅ Done! Processed {total_processed} images saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "Removing bad examples from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24850 images to clean. Starting process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24850/24850 [00:08<00:00, 3003.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Kept 22864 images and discarded 1986.\n",
      "Cleaned images are saved in ./data/cleaned/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration: TUNE THESE VALUES ---\n",
    "SOURCE_DIR = './data/thresholded/'\n",
    "OUTPUT_DIR = './data/cleaned/'\n",
    "\n",
    "# -- Contour Filters --\n",
    "MIN_CONTOURS = 1      # Minimum number of contours to be considered valid\n",
    "MAX_CONTOURS = 3     # Maximum number of contours (to filter out noisy images)\n",
    "\n",
    "# This filters out images that are mostly blank or mostly solid white\n",
    "MIN_AREA_RATIO = 0.20 \n",
    "MAX_AREA_RATIO = 0.80  \n",
    "\n",
    "def analyze_and_filter_image(relative_path):\n",
    "    try:\n",
    "        full_input_path = os.path.join(SOURCE_DIR, relative_path)\n",
    "        image = cv2.imread(full_input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None: return 0\n",
    "\n",
    "        # Find all contours in the binary image\n",
    "        contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # --- Apply Filtering Logic ---\n",
    "        # 1. Filter by number of contours\n",
    "        if not (MIN_CONTOURS <= len(contours) <= MAX_CONTOURS):\n",
    "            return 0 # Discard\n",
    "\n",
    "        # 2. Filter by the area of the largest contour\n",
    "        image_area = image.shape[0] * image.shape[1]\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        contour_area = cv2.contourArea(largest_contour)\n",
    "        area_ratio = contour_area / image_area\n",
    "        \n",
    "        if not (MIN_AREA_RATIO <= area_ratio <= MAX_AREA_RATIO):\n",
    "            return 0 # Discard\n",
    "        \n",
    "        # --- If all checks pass, copy the file ---\n",
    "        full_output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "        cv2.imwrite(full_output_path, image) # Using cv2.imwrite to copy\n",
    "        \n",
    "        return 1 # Keep\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# --- 2. Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(SOURCE_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), SOURCE_DIR)\n",
    "                all_image_paths.append(relative_path)\n",
    "\n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {SOURCE_DIR}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_image_paths)} images to clean. Starting process...\")\n",
    "\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(analyze_and_filter_image, all_image_paths), total=len(all_image_paths)))\n",
    "\n",
    "        total_kept = sum(results)\n",
    "        total_discarded = len(all_image_paths) - total_kept\n",
    "        print(f\"\\n✅ Done! Kept {total_kept} images and discarded {total_discarded}.\")\n",
    "        print(f\"Cleaned images are saved in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for dataset balance after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 35\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "Examples per class: [6490, 6930, 6730, 6890, 5870, 6540, 6990, 5960, 6700, 7120, 6560, 6590, 6890, 6250, 6510, 5360, 7020, 6920, 6050, 5180, 6940, 5850, 7140, 7140, 6780, 6530, 7140, 7100, 5540, 6850, 6990, 6780, 7030, 6110, 5150]\n",
      "Total examples: 228620\n",
      "Average examples per class: 6532.00\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Checking the training dataset\n",
    "\n",
    "# Checking the number of examples to guarantee balanced training\n",
    "base_path = \"./data/augmented_data/\" \n",
    "classes = [entry.name for entry in os.scandir(base_path)]\n",
    "print(f\"Number of classes: {len(classes)}\")\n",
    "\n",
    "# Count examples per class and create bar chart\n",
    "examples_count = [len(list(os.scandir(f\"{base_path}/{char}\"))) for char in classes]\n",
    "plt.bar(classes, examples_count)\n",
    "plt.title(\"Examples per Class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nClasses: {classes}\")\n",
    "print(f\"Examples per class: {examples_count}\")\n",
    "print(f\"Total examples: {sum(examples_count)}\")\n",
    "print(f\"Average examples per class: {sum(examples_count) / len(examples_count):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not perfect, but acceptable (max ratio of 1.4 : 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_PATH = \"./data/cleaned/\"\n",
    "SOURCE_DIR = BASE_PATH\n",
    "OUTPUT_DIR = './data/augmented_data'\n",
    "NUM_AUGMENTATIONS_PER_IMAGE = 10 # How many new versions to create for each original\n",
    "\n",
    "# These are good augmentations for character OCR. They are applied with some probability.\n",
    "transform = A.Compose([\n",
    "    A.Affine(\n",
    "        translate_percent=0.05,\n",
    "        scale=(0.95, 1.05),\n",
    "        rotate=(-10, 10),\n",
    "        p=0.8\n",
    "    ),\n",
    "    A.MotionBlur(blur_limit=5, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "])\n",
    "\n",
    "def augment_and_save(image_filename):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    filepath = os.path.join(SOURCE_DIR, image_filename)\n",
    "    try:\n",
    "        # Read the image using OpenCV\n",
    "        image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not read {image_filename}, skipping.\")\n",
    "            return 0\n",
    "        \n",
    "        original_output_path = os.path.join(OUTPUT_DIR, image_filename)\n",
    "        os.makedirs(os.path.dirname(original_output_path), exist_ok=True)\n",
    "        cv2.imwrite(original_output_path, image)\n",
    "\n",
    "        # Generate and save augmented images\n",
    "        saved_count = 1\n",
    "        base_filename = os.path.splitext(image_filename)[0]\n",
    "        \n",
    "        for i in range(NUM_AUGMENTATIONS_PER_IMAGE):\n",
    "            augmented = transform(image=image)\n",
    "            augmented_image = augmented['image']\n",
    "\n",
    "            new_filename = f\"{base_filename}_aug_{i}.png\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, new_filename)\n",
    "            \n",
    "            # Create subdirectory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            cv2.imwrite(output_path, augmented_image)\n",
    "            saved_count += 1\n",
    "        return saved_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_filename}: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folders = [entry.name for entry in os.scandir(BASE_PATH)]\n",
    "\n",
    "for folder_name in folders:\n",
    "  print(folder_name)\n",
    "\n",
    "  image_files = [f\"{folder_name}/{f}\" for f in os.listdir(SOURCE_DIR+folder_name) if f.endswith(('.png'))]\n",
    "\n",
    "  # Use ProcessPoolExecutor to run augmentations on all available CPU cores\n",
    "  with ProcessPoolExecutor() as executor:\n",
    "      results = executor.map(augment_and_save, image_files)\n",
    "\n",
    "  # total_saved = sum(results)\n",
    "  # print(f\"\\nGenerated a total of {total_saved} augmented images for class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the same preprocessing to eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10650 evaluation images to process. Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10650/10650 [00:04<00:00, 2338.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done! Successfully processed and saved 10650 images to ./data/processed_eval/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "SOURCE_DIR = './data/eval/'\n",
    "OUTPUT_DIR = './data/processed_eval/'\n",
    "NEW_SIZE = (40, 30)  # Must be the same size as your training images (width, height)\n",
    "\n",
    "# --- 1. Function to process a single image ---\n",
    "def process_image(relative_path):\n",
    "    \"\"\"\n",
    "    Reads, resizes, and applies Otsu's thresholding to a single image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct full input and output paths\n",
    "        full_input_path = os.path.join(SOURCE_DIR, relative_path)\n",
    "        full_output_path = os.path.join(OUTPUT_DIR, relative_path)\n",
    "\n",
    "        # Create the destination subdirectory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "        # Read the image in grayscale\n",
    "        image = cv2.imread(full_input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return 0  # Skip if image can't be read\n",
    "\n",
    "        # STEP 1: Resize the image\n",
    "        resized_image = cv2.resize(image, NEW_SIZE, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # STEP 2: Apply Otsu's Thresholding\n",
    "        processed_image = cv2.threshold(resized_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # Save the final processed image\n",
    "        cv2.imwrite(full_output_path, processed_image)\n",
    "        return 1  # Return 1 for success\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {relative_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# --- 2. Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Find all image files to process\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(SOURCE_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), SOURCE_DIR)\n",
    "                all_image_paths.append(relative_path)\n",
    "\n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {SOURCE_DIR}. Please check the path.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_image_paths)} evaluation images to process. Starting...\")\n",
    "\n",
    "        # Use ProcessPoolExecutor to run on all available CPU cores\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(process_image, all_image_paths), total=len(all_image_paths)))\n",
    "\n",
    "        total_processed = sum(results)\n",
    "        print(f\"\\n✅ Done! Successfully processed and saved {total_processed} images to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data formatting for training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251471 files belonging to 35 classes.\n",
      "Found 10650 files belonging to 35 classes.\n",
      "✅ Found 35 classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "✅ Data preparation complete and optimized.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 30\n",
    "IMG_WIDTH = 40\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "# --- Define paths ---\n",
    "caminho_train = 'data/augmented_data'\n",
    "caminho_eval = 'data/processed_eval'\n",
    "\n",
    "# --- Load Training and Validation Data ---\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    caminho_train,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    image_size=IMG_SHAPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    caminho_eval,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale',\n",
    "    image_size=IMG_SHAPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --- THE FIX: Get class_names BEFORE you modify the dataset ---\n",
    "class_names = train_ds.class_names\n",
    "print(f\"✅ Found {len(class_names)} classes: {class_names}\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- Normalize and Optimize ---\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"✅ Data preparation complete and optimized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction, training and eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modular CNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "I72w6YnovPpJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "\n",
    "def create_model(input_shape, num_classes, filters_per_layer=[32, 64], dense_units=128):\n",
    "    \"\"\"Creates a flexible CNN model.\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    # Convolutional blocks\n",
    "    for filters in filters_per_layer:\n",
    "        model.add(Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Classifier head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5)) # Dropout is a key regularizer\n",
    "    model.add(Dense(num_classes, activation='softmax', dtype='float32')) # Output layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8qJQ0Et1vXwk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Experiment: lr_1e-3 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4480</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">573,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,515</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4480\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m573,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)             │         \u001b[38;5;34m4,515\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">597,795</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m597,795\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">597,347</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m597,347\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m   1/7859\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:17:32\u001b[0m 7s/step - accuracy: 0.0000e+00 - loss: 4.8070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 04:14:15.750580: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 22118400 exceeds 10% of free system memory.\n",
      "2025-07-31 04:14:15.750839: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 22118400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   2/7859\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:38\u001b[0m 165ms/step - accuracy: 0.0078 - loss: 4.6635    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 04:14:16.012570: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 22118400 exceeds 10% of free system memory.\n",
      "2025-07-31 04:14:16.023301: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 22118400 exceeds 10% of free system memory.\n",
      "2025-07-31 04:14:16.191650: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 22118400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1296/7859\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:49\u001b[0m 117ms/step - accuracy: 0.7937 - loss: 0.7886"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     29\u001b[39m os.makedirs(os.path.dirname(model_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m callbacks = [\n\u001b[32m     32\u001b[39m     TensorBoard(log_dir=log_dir),\n\u001b[32m     33\u001b[39m     ModelCheckpoint(filepath=model_path, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     34\u001b[39m     EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     35\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set high, EarlyStopping will handle it\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:378\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    376\u001b[39m callbacks.on_train_batch_begin(begin_step)\n\u001b[32m    377\u001b[39m logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/callbacks/callback_list.py:172\u001b[39m, in \u001b[36mCallbackList.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m._async_dispatch(\u001b[38;5;28mself\u001b[39m._on_train_batch_end, batch, logs)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/callbacks/callback_list.py:194\u001b[39m, in \u001b[36mCallbackList._on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    192\u001b[39m logs = python_utils.pythonify_logs(logs)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[43mcallback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/callbacks/progbar_logger.py:58\u001b[39m, in \u001b[36mProgbarLogger.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/callbacks/progbar_logger.py:95\u001b[39m, in \u001b[36mProgbarLogger._update_progbar\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.seen = batch + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprogbar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/utils/progbar.py:166\u001b[39m, in \u001b[36mProgbar.update\u001b[39m\u001b[34m(self, current, values, finalize)\u001b[39m\n\u001b[32m    163\u001b[39m info += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._values[k], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    165\u001b[39m     avg = backend.convert_to_numpy(\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    170\u001b[39m     avg = \u001b[38;5;28mfloat\u001b[39m(avg)\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(avg) > \u001b[32m1e-3\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py:717\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(x, axis, keepdims)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    715\u001b[39m     result_dtype = ori_dtype\n\u001b[32m    716\u001b[39m output = tf.reduce_mean(\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m)\u001b[49m, axis=axis, keepdims=keepdims\n\u001b[32m    718\u001b[39m )\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tf.cast(output, result_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/alpr/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:138\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.version_info.major != \u001b[32m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m sys.version_info.minor < \u001b[32m7\u001b[39m:\n\u001b[32m    136\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_handler\u001b[39m(*args, **kwargs):\n\u001b[32m    139\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_traceback_filtering_enabled():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# --- Setup ---\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "NUM_CLASSES = len(class_names) # Get number of classes from the loaded data\n",
    "\n",
    "# --- Define Your Experiments ---\n",
    "experiments = [\n",
    "    {'id': 'lr_1e-3', 'learning_rate': 1e-3},\n",
    "    {'id': 'lr_5e-4', 'learning_rate': 5e-4},\n",
    "    {'id': 'lr_1e-4', 'learning_rate': 1e-4},\n",
    "]\n",
    "\n",
    "for config in experiments:\n",
    "    print(f\"\\n--- Running Experiment: {config['id']} ---\")\n",
    "\n",
    "    model = create_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    log_dir = os.path.join(\"logs\", config['id'])\n",
    "    model_path = os.path.join(\"models\", f\"{config['id']}_best_model.keras\")\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        TensorBoard(log_dir=log_dir),\n",
    "        ModelCheckpoint(filepath=model_path, save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "        EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=100, # Set high, EarlyStopping will handle it\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2682,
     "status": "ok",
     "timestamp": 1710973815131,
     "user": {
      "displayName": "Enzo Nicolás Stromberg Racciatti",
      "userId": "12238508451821531070"
     },
     "user_tz": 180
    },
    "id": "CkKCvqkz2_56",
    "outputId": "743d0ace-ef5b-409c-e0a1-6fed9c195128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - 8ms/step - accuracy: 0.9860 - loss: 0.1554\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# --- Evaluate the Best Model ---\n",
    "# Find which model performed best by checking the logs or filenames.\n",
    "# Let's assume the 'lr_5e-4' model was the best.\n",
    "best_model_path = 'models/lr_5e-4_best_model.keras'\n",
    "\n",
    "print(f\"Loading best model from: {best_model_path}\")\n",
    "loaded_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Evaluate the loaded model on your actual evaluation data\n",
    "print(\"\\nEvaluating on the processed evaluation dataset:\")\n",
    "loss, accuracy = loaded_model.evaluate(val_ds, verbose=2)\n",
    "\n",
    "print(f\"\\nEvaluation Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training efficiency increase\n",
    "Quantization, ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONzSl3H77Q5Gqz5FnyphQ3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
